{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDPBot\n",
    "\n",
    "---\n",
    "\n",
    "A bot based on BERT using transfer learning\n",
    "The dataset to train and test the model will be available of interact in english, but no in spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\BertBot\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "e:\\BertBot\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\carlo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.43MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 378kB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:06<00:00, 68.3MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load bert tokenizer and Classification from transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to tokenize the texts that are going to be used for train and test the model in this case, i am going to use TriviaQA from http://nlp.cs.washington.edu/triviaqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "# Download dataset if it doesn't exist\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "url = \"http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\"\n",
    "target_path = 'triviaqa-unfiltered.tar.gz'\n",
    "\n",
    "if target_path not in os.listdir():\n",
    "    print(\"Downloading file\")\n",
    "    # Dowload the file showing the progress bar\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Saving file\")\n",
    "        with open(target_path, 'wb') as f:\n",
    "            f.write(response.raw.read())\n",
    "\n",
    "    # Unzip the file\n",
    "    with tarfile.open(target_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Welcome to GoComics.com, the world's largest c...   \n",
      "1  The Nobel Prize in Literature 1930 Sinclair .....   \n",
      "2  Dame Judi Dench is a renowned ... Born in Engl...   \n",
      "3  Our expert has answerd your question. Meet our...   \n",
      "4  Third Man Records Launches First Record Played...   \n",
      "\n",
      "                                            question  answers  \n",
      "0  Who was President when the first Peanuts carto...      NaN  \n",
      "1  Which American-born Sinclair won the Nobel Pri...      NaN  \n",
      "2         Where in England was Dame Judi Dench born?      NaN  \n",
      "3  William Christensen of Madison, New Jersey, ha...      NaN  \n",
      "4  In which decade did Billboard magazine first p...      NaN  \n"
     ]
    }
   ],
   "source": [
    "# create a dataframe from the json file with the variables that i need \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "if 'triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-train.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'Question': question,\n",
    "                'Answer': answer,\n",
    "                'Context': context\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        df.to_csv('triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    df = pd.read_csv('triviaqa-unfiltered.csv')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Alice Cooper's The Man Behind the Mask Music V...   \n",
      "1  Jamie Lee Curtis, Actress: True Lies. Jamie Le...   \n",
      "2  The official website for Andrew Lloyd Webber, ...   \n",
      "3  The history and complete text of the 1917 Balf...   \n",
      "4  ... credits and award information for 70 Numbe...   \n",
      "\n",
      "                                            question             answers  \n",
      "0              Who was the man behind The Chipmunks?       David Seville  \n",
      "1                What star sign is Jamie Lee Curtis?             Scorpio  \n",
      "2  Which Lloyd Webber musical premiered in the US...    Sunset Boulevard  \n",
      "3  Who was the next British Prime Minister after ...  Campbell-Bannerman  \n",
      "4     Who had a 70s No 1 hit with Kiss You All Over?               Exile  \n"
     ]
    }
   ],
   "source": [
    "if 'test-triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-dev.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'Question': question,\n",
    "                'Answer': answer,\n",
    "                'Context': context\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        test = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        test.to_csv('test-triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    test = pd.read_csv('test-triviaqa-unfiltered.csv')\n",
    "    print(test.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
